---
title: "Appendix C: Analysis Pipeline 2 - RFE-CV Feature Selection"
author: "Deevanshi Walia"
date: "`r Sys.Date()`"
output:
  word_document:
    toc: yes
    toc_depth: 3
---

```{r setup_pipeline_3, include=FALSE}
# Global setup chunk for this document
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)

# Load all libraries that will be used throughout the document
library(readr); library(dplyr); library(caret); library(tidyr); library(MASS);
library(pROC); library(ggplot2); library(randomForest); library(e1071);
library(xgboost); library(GGally); library(tibble); library(ggpubr); library(forcats);
library(knitr)
```

# 1. Cross-Validated Feature Selection (RFE-CV)

This section details the loading of the feature matrix and the use of a rigorous, 10-fold cross-validated feature selection pipeline (RFE-CV) based on the Wilcoxon rank-sum test to derive a stable biomarker signature.

```{r p3_feature_selection}
# --- PART A: CONFIGURATION ---
feature_matrix_path <- "C:/Users/deevanshi.walia/Desktop/Try 2.0/Onset of Symptoms/Final_Analysis_Output_Peaks_Clustering_9_0.05/Feature_Matrices/feature_matrix_ROI_INTEGRATION.csv"
NUM_FOLDS <- 10
SELECTION_STABILITY_THRESHOLD <- 0.8

# --- PART B: LOAD DATA ---
data_full <- read_csv(feature_matrix_path, show_col_types = FALSE)
data_full$GroupName <- as.factor(data_full$GroupName)
if (any(is.na(data_full))) { data_full <- na.omit(data_full) }

# --- PART C: RFE-CV ---
set.seed(123)
folds <- createFolds(data_full$GroupName, k = NUM_FOLDS, list = TRUE)
list_of_fold_results <- lapply(1:NUM_FOLDS, function(i) {
  train_data <- data_full[setdiff(1:nrow(data_full), folds[[i]]), ]
  feature_data_train <- train_data %>% dplyr::select(starts_with("Cluster_"))
  lapply(names(feature_data_train), function(cn) {
    p_val <- tryCatch(wilcox.test(train_data[[cn]] ~ train_data$GroupName)$p.value, error=function(e) 1.0)
    data.frame(Feature = cn, p.value = p_val)
  }) %>% bind_rows()
})
all_fold_p_values <- bind_rows(list_of_fold_results, .id="Fold")
p_value_summary <- all_fold_p_values %>%
  group_by(Feature) %>%
  summarise(Selection_Count = sum(p.value < 0.05, na.rm = TRUE)) %>%
  arrange(desc(Selection_Count))

selected_features <- p_value_summary %>%
  filter(Selection_Count >= (SELECTION_STABILITY_THRESHOLD * NUM_FOLDS)) %>%
  pull(Feature)

if (length(selected_features) == 0) {
  knitr::knit_exit("RFE-CV resulted in zero features meeting the stability threshold.")
}
feature_data_best <- data_full %>% dplyr::select(all_of(selected_features))
metadata <- data_full %>% dplyr::select(SampleName, GroupName)
```

The RFE-CV process identified **`r length(selected_features)`** features that were statistically significant (p < 0.05) in at least `r SELECTION_STABILITY_THRESHOLD * 100`% of the cross-validation folds. The full stability summary and the final selected signature are shown below.

### Feature Selection Stability Summary
```{r p3_print_stability_summary}
kable(p_value_summary, caption = "Frequency of feature selection across 10 cross-validation folds.")
```

**Final Robust Feature Signature:** `r paste(selected_features, collapse=", ")`.

# 2. Predictive Validation of the RFE-CV Signature

The final biomarker signature was then subjected to a rigorous validation pipeline to assess its real-world predictive performance.

```{r p3_model_gauntlet}
# --- This chunk performs the LOOCV and creates the summary objects ---
library(MASS); library(randomForest); library(e1071); library(xgboost); library(pROC); library(caret)

# --- LOOCV Model Gauntlet ---
num_samples <- nrow(feature_data_best); y_response <- as.factor(metadata$GroupName)
y_response_numeric <- as.numeric(y_response)-1; current_group_levels <- levels(y_response)
positive_class_name <- current_group_levels[2]
loocv_results <- data.frame(SampleName=metadata$SampleName, ActualGroup=y_response,
                            lda_scores=numeric(num_samples), rf_scores=numeric(num_samples),
                            svm_scores=numeric(num_samples), xgb_scores=numeric(num_samples))
for (i in 1:num_samples) {
  x_train <- as.matrix(feature_data_best[-i, , drop=FALSE]); y_train <- y_response[-i]
  y_train_numeric <- y_response_numeric[-i]; x_test <- as.matrix(feature_data_best[i, , drop = FALSE])
  
  lda_model<-lda(x=x_train, grouping=y_train); loocv_results$lda_scores[i]<-predict(lda_model,newdata=data.frame(x_test))$posterior[,positive_class_name]
  min_class_size<-min(table(y_train)); rf_model<-randomForest(x=x_train,y=y_train,ntree=500,sampsize=c(min_class_size,min_class_size))
  loocv_results$rf_scores[i]<-predict(rf_model,newdata=x_test,type="prob")[,positive_class_name]
  svm_model<-svm(x=x_train,y=y_train,probability=T,kernel="radial"); svm_pred<-predict(svm_model,newdata=data.frame(x_test),probability=T)
  loocv_results$svm_scores[i]<-attr(svm_pred,"probabilities")[,positive_class_name]
  dtrain<-xgb.DMatrix(data=x_train,label=y_train_numeric); dtest<-xgb.DMatrix(data=x_test)
  scale_pos_weight<-sum(y_train_numeric==0)/sum(y_train_numeric==1)
  params<-list(objective="binary:logistic",eval_metric="logloss",scale_pos_weight=scale_pos_weight)
  xgb_model<-xgboost(params=params,data=dtrain,nrounds=50,verbose=0); loocv_results$xgb_scores[i]<-predict(xgb_model,dtest)
}
# --- Performance Comparison ---
roc_lda <- roc(response=loocv_results$ActualGroup, predictor=loocv_results$lda_scores, levels=current_group_levels)
roc_rf <- roc(response=loocv_results$ActualGroup, predictor=loocv_results$rf_scores, levels=current_group_levels)
roc_svm <- roc(response=loocv_results$ActualGroup, predictor=loocv_results$svm_scores, levels=current_group_levels)
roc_xgb <- roc(response=loocv_results$ActualGroup, predictor=loocv_results$xgb_scores, levels=current_group_levels)
model_performance_list <- list(); models_to_test <- list(LDA=roc_lda, `Random Forest`=roc_rf, SVM=roc_svm, XGBoost=roc_xgb); cm_list <- list() 
for (model_name in names(models_to_test)) {
  current_roc <- models_to_test[[model_name]]
  best_coords <- coords(current_roc, "best", ret="threshold", best.method="youden")
  predicted_classes <- ifelse(current_roc$predictor >= best_coords$threshold, current_group_levels[2], current_group_levels[1])
  cm_stats <- confusionMatrix(data=factor(predicted_classes, levels=current_group_levels), reference=current_roc$response, positive=positive_class_name)
  cm_list[[model_name]] <- cm_stats
  model_performance_list[[model_name]] <- c(AUC=auc(current_roc), cm_stats$overall["Accuracy"], cm_stats$byClass[c("Sensitivity","Specificity","Balanced Accuracy","Pos Pred Value","Neg Pred Value")])
}
performance_summary <- as.data.frame(do.call(rbind, model_performance_list)) %>%
  tibble::rownames_to_column("Model") %>%
  mutate(across(where(is.numeric), ~ round(., 4))) %>%
  dplyr::select(Model, AUC, Accuracy, `Balanced Accuracy`, Sensitivity, Specificity, `Pos Pred Value`, `Neg Pred Value`)
```

### 2.1. Comparative Model Performance

```{r print_p3_performance_summary}
kable(performance_summary, caption = "Comparative performance metrics for classifiers trained on the RFE-CV selected signature.")
```

### 2.2. ROC Curve and Best Model Analysis

```{r plot_p3_roc_and_cm}
# --- Combined ROC Curve Plot ---
roc_list_for_plot <- list(LDA=roc_lda, RF=roc_rf, SVM=roc_svm, XGBoost=roc_xgb)
ggroc(roc_list_for_plot, size = 1) +
  annotate("segment", x = 1, xend = 0, y = 0, yend = 1, color="grey", linetype="dashed") +
  labs(title = "Comparative ROC Curves", color = "Model") +
  theme_minimal() + theme(plot.title = element_text(face = "bold", hjust = 0.5))

# --- Best Model's Confusion Matrix ---
best_model_name <- performance_summary$Model[which.max(performance_summary$`Balanced Accuracy`)]
cat("\n--- Detailed Confusion Matrix for the Best Performing Model:", best_model_name, "---\n")
# Get the confusion matrix object for the best model
best_cm_object <- cm_list[[best_model_name]]

# --- 1. Create the main confusion matrix table ---
cm_table <- as.data.frame(best_cm_object$table)
colnames(cm_table) <- c("Prediction", "Reference", "Frequency")
cm_table_wide <- tidyr::pivot_wider(cm_table, names_from = "Reference", values_from = "Frequency")

# Use kable() to format it nicely
knitr::kable(
  cm_table_wide,
  caption = paste("Confusion Matrix for the", best_model_name, "Model")
)
# --- 2. Create a separate table for the key statistics ---
# Extract the key metrics we want to show
key_stats <- c(
  "Accuracy",
  "Kappa",
  "Balanced Accuracy",
  "Sensitivity",
  "Specificity",
  "Pos Pred Value",
  "Neg Pred Value"
)
# Combine overall and by-class stats
all_stats <- c(best_cm_object$overall, best_cm_object$byClass)

# Create a clean data frame of the stats
stats_to_display <- data.frame(
  Metric = key_stats,
  Value = round(all_stats[key_stats], 4) # Round to 4 decimal places
)

# Use kable() to format this table as well
knitr::kable(
  stats_to_display,
  caption = "Key Performance Statistics for the Best Model"
)
```

# 3. Intensity Distributions
```{r feature interactions, fig.width=12, fig.height=12}
library(ggpubr)
library(GGally)
library(ggplot2)
# A pairs plot is most useful if you have between 2 and ~6 features.
if (length(selected_features) >= 2) {
  
  # 1. Select the data for the plot
  # This includes the GroupName and all the selected cluster intensities
  data_for_pairs_plot <- cbind(
    GroupName = metadata$GroupName,
    feature_data_best[, selected_features]
  )

# --- Create the pairs plot with improved theme settings ---
pairs_plot <- ggpairs(
  data_for_pairs_plot,
  mapping = aes(color = GroupName, alpha = 0.7),
  columns = 2:ncol(data_for_pairs_plot),
  lower = list(continuous = wrap("points", size = 2)),
  diag = list(continuous = wrap("densityDiag", alpha = 0.5)),
  upper = list(continuous = "blank")
) +
  labs(title = paste("Pairwise Interactions of", length(selected_features), "Selected Features")) +
  
  theme_bw(base_size = 10) + 
  theme(
    # Center the main plot title
    plot.title = element_text(face="bold", size=16, hjust=0.5),
    
    # Adjust the panel strip text (the "Cluster_X" labels at the top and right)
    strip.text.x = element_text(
      size = 10, # Make the font smaller
      angle = 0, # Angle the text to prevent overlap
      hjust = 0.5,# Adjust horizontal alignment
      face= "bold" 
    ),
    strip.text.y = element_text(
      size = 10,
      angle = 90, # Keep y-axis labels horizontal
      hjust = 0.5, # Adjust horizontal alignment
      face="bold"   
    ),
    
    # Give the axis labels a bit more room
    axis.text.x = element_text(size=7, angle=45, vjust=0.5),
    axis.text.y = element_text(size=7),
    
    # Move the legend to the bottom to free up space on the right
    legend.position = "bottom"
  )

# This will now embed the cleaner, more readable plot
pairs_plot
 
} else{
  print("   - Skipping pairs plot: Fewer than 2 features were selected.\n")
  # If only one feature is selected, a boxplot is the most appropriate visualization
  plot_data_long <- data_full %>%
  dplyr::select(GroupName, all_of(selected_features)) %>%
  pivot_longer(
    cols = -GroupName,
    names_to = "Feature",
    values_to = "Intensity"
  ) %>%
  # A more robust way to reorder the features numerically for the plot
  mutate(Feature = fct_reorder(Feature, as.numeric(gsub("Cluster_", "", Feature))))

# --- Create the plot with improved aesthetics ---
boxplots_with_pvals <- ggplot(
  data = plot_data_long,
  aes(x = GroupName, y = Intensity, fill = GroupName)
) +
  geom_boxplot(alpha = 0.7, outlier.shape = NA) +
  geom_jitter(width = 0.2, alpha = 0.4, size = 1.5) +
  
  # Add Wilcoxon p-values to each panel
  stat_compare_means(
    method = "wilcox.test",
    label = "p.format", # e.g., "p = 0.04"
    label.x.npc = 0.5,  # Center the label
    size = 3.5
  ) +
  
  # Create a separate panel for each feature
  facet_wrap(~ Feature, scales = "free_y", ncol = 4) + # Arrange in 4 columns
  
  # --- EXPLICITLY DEFINE YOUR COLORS ---
  scale_fill_manual(
    name = "Group",
    values = c("Group 1 (pos)" = "darkgreen","Group 2 (neg)" = "darkorange") # Example: Blue and Yellow
    # â— IMPORTANT: Make sure these group names exactly match the levels in your `GroupName` column
  ) +
  
  labs(
    title = "Intensity Distribution of Boruta-Selected Features",
    subtitle = "P-values from Wilcoxon Rank-Sum Test",
    x = NULL, # Remove redundant "Group" label
    y = "Intensity (a.u)"
  ) +
  
  theme_bw(base_size = 12) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5),
    strip.text = element_text(face = "bold"),
    legend.position = "none",
    axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1)
  )

# This is the final line, which tells knitr to render the plot
boxplots_with_pvals
}
```
